{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"13Kdw6fItVZn5LbxIbvenM0uV5DRE4m3y","timestamp":1680204809948}],"authorship_tag":"ABX9TyMANnzNoBpMUNQzOI0iJKri"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"953243c2e88543698b8e9c14a10a241d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f5f169f4ba914782a904aa554de5ae64","IPY_MODEL_bd2e6a9750a44550acdc6c0a7490b4be","IPY_MODEL_88621a74f69d493fa4a4156fd4b6321d"],"layout":"IPY_MODEL_bce0e12dce314017ba790ab4ba7a6ed2"}},"f5f169f4ba914782a904aa554de5ae64":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef66096ec9b845b9bc903c8973607b95","placeholder":"​","style":"IPY_MODEL_5096e30ecc4f4849859e73b1d6096f89","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"bd2e6a9750a44550acdc6c0a7490b4be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc30c1a4b1ad4fdb98612f16a666a0d5","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_54360539fa734d199cec3de15c63aede","value":231508}},"88621a74f69d493fa4a4156fd4b6321d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f687f1fa4874ba0a8213ca9bfde1d7c","placeholder":"​","style":"IPY_MODEL_fcf3a683d3864546ad8d89615c447292","value":" 232k/232k [00:00&lt;00:00, 336kB/s]"}},"bce0e12dce314017ba790ab4ba7a6ed2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef66096ec9b845b9bc903c8973607b95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5096e30ecc4f4849859e73b1d6096f89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc30c1a4b1ad4fdb98612f16a666a0d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54360539fa734d199cec3de15c63aede":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f687f1fa4874ba0a8213ca9bfde1d7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcf3a683d3864546ad8d89615c447292":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"686c0292ab3741f8b6157b9b69e48ed9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c959f31747c2479f969afb270ebb5922","IPY_MODEL_fb274c25c51745e8b05fdb18833cb07b","IPY_MODEL_388fb257e9b64bc9a76cf8b8af929f50"],"layout":"IPY_MODEL_bad9803a18a549858bb21ccf1bab3b16"}},"c959f31747c2479f969afb270ebb5922":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e7795f57a724b95a0d19856b2e21814","placeholder":"​","style":"IPY_MODEL_6be63d6c4d4344869f8c387f687f3677","value":"Downloading (…)okenizer_config.json: 100%"}},"fb274c25c51745e8b05fdb18833cb07b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d8ea43b0bbc441d8ae956b02f2b9912","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b85698d5cf4c4dd580baa962b81efa75","value":28}},"388fb257e9b64bc9a76cf8b8af929f50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_199740d428ec4af38aa66f52f8a2c235","placeholder":"​","style":"IPY_MODEL_f338a47c4a0d4aecbffe74d8f86d0351","value":" 28.0/28.0 [00:00&lt;00:00, 1.28kB/s]"}},"bad9803a18a549858bb21ccf1bab3b16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e7795f57a724b95a0d19856b2e21814":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6be63d6c4d4344869f8c387f687f3677":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d8ea43b0bbc441d8ae956b02f2b9912":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b85698d5cf4c4dd580baa962b81efa75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"199740d428ec4af38aa66f52f8a2c235":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f338a47c4a0d4aecbffe74d8f86d0351":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"150eab7b25cd4d5b82f1629a8bd89071":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0b8377a69d54606866efcda8485c772","IPY_MODEL_e187d25ba89043a4942a12cd4bf72f0d","IPY_MODEL_be517f3e334e43578c01206ce391c9e7"],"layout":"IPY_MODEL_126e95d81d5b480bb25266b8e915c0b8"}},"e0b8377a69d54606866efcda8485c772":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45a7fcbbbb504345b622f03bd74d09ce","placeholder":"​","style":"IPY_MODEL_8dbee1839ea74cb0ba4e8c8debd939c7","value":"Downloading (…)lve/main/config.json: 100%"}},"e187d25ba89043a4942a12cd4bf72f0d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f70ff38548648af9247792a50135f46","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92bec8d5eb5c48329f58d71c88daf792","value":570}},"be517f3e334e43578c01206ce391c9e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3eff18526ce48298eda4140647816e2","placeholder":"​","style":"IPY_MODEL_58e29d93ad9a441dbd722e537237ae02","value":" 570/570 [00:00&lt;00:00, 10.5kB/s]"}},"126e95d81d5b480bb25266b8e915c0b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45a7fcbbbb504345b622f03bd74d09ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dbee1839ea74cb0ba4e8c8debd939c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f70ff38548648af9247792a50135f46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92bec8d5eb5c48329f58d71c88daf792":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3eff18526ce48298eda4140647816e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58e29d93ad9a441dbd722e537237ae02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c77359dc32784f80a2cc71afa805d83e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f36b9160ed61440ab2e3df73d6c3c1e1","IPY_MODEL_2da08eb3513944e0af332fe21275adbf","IPY_MODEL_19cae040b8214186ba8af5cf2f4cf71a"],"layout":"IPY_MODEL_34f16c4535fc4ab89ee76919929d33c3"}},"f36b9160ed61440ab2e3df73d6c3c1e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5acff903f5d45dbb5ff68809ea088a7","placeholder":"​","style":"IPY_MODEL_529cce803178418a98c07ebc3a1e32c0","value":"Downloading pytorch_model.bin: 100%"}},"2da08eb3513944e0af332fe21275adbf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a88fc83da89465f956b6afd496dcefe","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a09d3c1c18ba4fbead2c9786bbdb14ce","value":440473133}},"19cae040b8214186ba8af5cf2f4cf71a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fab028ef37524e268ac4655c358db262","placeholder":"​","style":"IPY_MODEL_e0ab32b121234a22abb36703b60d4988","value":" 440M/440M [00:04&lt;00:00, 95.1MB/s]"}},"34f16c4535fc4ab89ee76919929d33c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5acff903f5d45dbb5ff68809ea088a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"529cce803178418a98c07ebc3a1e32c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a88fc83da89465f956b6afd496dcefe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a09d3c1c18ba4fbead2c9786bbdb14ce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fab028ef37524e268ac4655c358db262":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0ab32b121234a22abb36703b60d4988":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JWZYwTNP6rMB","executionInfo":{"status":"ok","timestamp":1680212360479,"user_tz":240,"elapsed":3313,"user":{"displayName":"Yunjie Qian","userId":"16160804486899806073"}},"outputId":"42a09012-c04c-4433-ff7f-517d56cc7671"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":1}],"source":["import torch\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"]},{"cell_type":"code","source":["!pip install transformers wget"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dy82adc77CwF","executionInfo":{"status":"ok","timestamp":1680212373995,"user_tz":240,"elapsed":13518,"user":{"displayName":"Yunjie Qian","userId":"16160804486899806073"}},"outputId":"5f9156ea-0b73-49fd-cfe1-32c82287e89d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=ab36cfc6fd7d8a2495e3e9d2d6b4d51db573aff6ac1eaf02d879b380e461ff08\n","  Stored in directory: /root/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n","Successfully built wget\n","Installing collected packages: wget, tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4 wget-3.2\n"]}]},{"cell_type":"code","source":["import wget, os\n","\n","url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n","\n","if not os.path.exists('./cola_public_1.1.zip'):\n","    wget.download(url, './cola_public_1.1.zip')\n","\n","if not os.path.exists('./cola_public/'):\n","    !unzip cola_public_1.1.zip"],"metadata":{"id":"wggw5OnS7Jly","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680212374904,"user_tz":240,"elapsed":912,"user":{"displayName":"Yunjie Qian","userId":"16160804486899806073"}},"outputId":"b16dbeac-7c9c-4730-df8e-fd0c21143fb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  cola_public_1.1.zip\n","   creating: cola_public/\n","  inflating: cola_public/README      \n","   creating: cola_public/tokenized/\n","  inflating: cola_public/tokenized/in_domain_dev.tsv  \n","  inflating: cola_public/tokenized/in_domain_train.tsv  \n","  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n","   creating: cola_public/raw/\n","  inflating: cola_public/raw/in_domain_dev.tsv  \n","  inflating: cola_public/raw/in_domain_train.tsv  \n","  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, \n","                 names=['sentence_source', 'label', 'label_notes', 'sentence'])\n","sentences, labels = list(df.sentence.values), df.label.values\n","\n","print('Number of training sentences: {}\\n'.format(len(df)))\n","df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"id":"DJhsdgB-7JuF","executionInfo":{"status":"ok","timestamp":1680212375558,"user_tz":240,"elapsed":656,"user":{"displayName":"Yunjie Qian","userId":"16160804486899806073"}},"outputId":"83c7f12d-a21e-4274-c032-a0f6822cd254"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training sentences: 8551\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["  sentence_source  label label_notes  \\\n","0            gj04      1         NaN   \n","1            gj04      1         NaN   \n","2            gj04      1         NaN   \n","3            gj04      1         NaN   \n","4            gj04      1         NaN   \n","\n","                                            sentence  \n","0  Our friends won't buy this analysis, let alone...  \n","1  One more pseudo generalization and I'm giving up.  \n","2   One more pseudo generalization or I'm giving up.  \n","3     The more we study verbs, the crazier they get.  \n","4          Day by day the facts are getting murkier.  "],"text/html":["\n","  <div id=\"df-317a1258-8c3d-4a28-8a3e-49705fa77273\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_source</th>\n","      <th>label</th>\n","      <th>label_notes</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>gj04</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>Our friends won't buy this analysis, let alone...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>gj04</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>One more pseudo generalization and I'm giving up.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>gj04</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>One more pseudo generalization or I'm giving up.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>gj04</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>The more we study verbs, the crazier they get.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>gj04</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>Day by day the facts are getting murkier.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-317a1258-8c3d-4a28-8a3e-49705fa77273')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-317a1258-8c3d-4a28-8a3e-49705fa77273 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-317a1258-8c3d-4a28-8a3e-49705fa77273');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","print('Original: ', sentences[0])\n","print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167,"referenced_widgets":["953243c2e88543698b8e9c14a10a241d","f5f169f4ba914782a904aa554de5ae64","bd2e6a9750a44550acdc6c0a7490b4be","88621a74f69d493fa4a4156fd4b6321d","bce0e12dce314017ba790ab4ba7a6ed2","ef66096ec9b845b9bc903c8973607b95","5096e30ecc4f4849859e73b1d6096f89","dc30c1a4b1ad4fdb98612f16a666a0d5","54360539fa734d199cec3de15c63aede","3f687f1fa4874ba0a8213ca9bfde1d7c","fcf3a683d3864546ad8d89615c447292","686c0292ab3741f8b6157b9b69e48ed9","c959f31747c2479f969afb270ebb5922","fb274c25c51745e8b05fdb18833cb07b","388fb257e9b64bc9a76cf8b8af929f50","bad9803a18a549858bb21ccf1bab3b16","6e7795f57a724b95a0d19856b2e21814","6be63d6c4d4344869f8c387f687f3677","2d8ea43b0bbc441d8ae956b02f2b9912","b85698d5cf4c4dd580baa962b81efa75","199740d428ec4af38aa66f52f8a2c235","f338a47c4a0d4aecbffe74d8f86d0351","150eab7b25cd4d5b82f1629a8bd89071","e0b8377a69d54606866efcda8485c772","e187d25ba89043a4942a12cd4bf72f0d","be517f3e334e43578c01206ce391c9e7","126e95d81d5b480bb25266b8e915c0b8","45a7fcbbbb504345b622f03bd74d09ce","8dbee1839ea74cb0ba4e8c8debd939c7","4f70ff38548648af9247792a50135f46","92bec8d5eb5c48329f58d71c88daf792","c3eff18526ce48298eda4140647816e2","58e29d93ad9a441dbd722e537237ae02"]},"id":"k0Z6sune7s-k","executionInfo":{"status":"ok","timestamp":1680212383920,"user_tz":240,"elapsed":8365,"user":{"displayName":"Yunjie Qian","userId":"16160804486899806073"}},"outputId":"2b3b2468-be67-4dde-d107-39cf7015fd6e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"953243c2e88543698b8e9c14a10a241d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"686c0292ab3741f8b6157b9b69e48ed9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"150eab7b25cd4d5b82f1629a8bd89071"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Original:  Our friends won't buy this analysis, let alone the next one we propose.\n","Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n","Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"]}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to their word IDs.\n","sentence_encodings = tokenizer(\n","                          sentences,                      # Sentence to encode\n","                          add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","                          truncation = True,              # Truncate a long sentence\n","                          padding = 'longest',            # Pad a short sentence\n","                          return_attention_mask = True,   # Construct attention mask\n","                          return_tensors = 'pt',          # Return pytorch tensors\n","                     )\n","\n","# Convert the lists into tensors.\n","input_ids = sentence_encodings['input_ids']\n","attention_masks = sentence_encodings['attention_mask']\n","labels = torch.tensor(labels)\n","\n","print(input_ids.shape, attention_masks.shape, labels.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XVcxuOA88Eaj","executionInfo":{"status":"ok","timestamp":1680212387729,"user_tz":240,"elapsed":3811,"user":{"displayName":"Yunjie Qian","userId":"16160804486899806073"}},"outputId":"091964b3-64d5-4f88-cda7-6df563c0ef79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8551, 47]) torch.Size([8551, 47]) torch.Size([8551])\n"]}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, random_split\n","\n","class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    \n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = self.labels[idx]\n","        return item\n","    \n","    def __len__(self):\n","        return len(self.labels)\n","\n","dataset = MyDataset(sentence_encodings, labels)\n","# dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# Create a 90-10 train-validation split.\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{} training samples'.format(train_size))\n","print('{} validation samples'.format(val_size))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u0LL7FDK8MjM","outputId":"d408b39c-6335-40d9-cec7-6a401ef40138","executionInfo":{"status":"ok","timestamp":1680212388114,"user_tz":240,"elapsed":393,"user":{"displayName":"Yunjie Qian","userId":"16160804486899806073"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7695 training samples\n","856 validation samples\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 32\n","\n","# Take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,\n","            sampler = RandomSampler(train_dataset),\n","            batch_size = batch_size\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset,\n","            sampler = SequentialSampler(val_dataset),\n","            batch_size = batch_size\n","        )"],"metadata":{"id":"nTnVgcBJ8Mpo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, BertConfig\n","\n","# Load a pretrained BERT model with a single linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\",          # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2,               # The number of output labels.\n","    output_attentions = False,    # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Run on GPU\n","model.to(device)\n","\n","[name for name, param in model.named_parameters()]"],"metadata":{"id":"128MTs4s8Mwq","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c77359dc32784f80a2cc71afa805d83e","f36b9160ed61440ab2e3df73d6c3c1e1","2da08eb3513944e0af332fe21275adbf","19cae040b8214186ba8af5cf2f4cf71a","34f16c4535fc4ab89ee76919929d33c3","f5acff903f5d45dbb5ff68809ea088a7","529cce803178418a98c07ebc3a1e32c0","5a88fc83da89465f956b6afd496dcefe","a09d3c1c18ba4fbead2c9786bbdb14ce","fab028ef37524e268ac4655c358db262","e0ab32b121234a22abb36703b60d4988"]},"executionInfo":{"status":"ok","timestamp":1680212405742,"user_tz":240,"elapsed":17630,"user":{"displayName":"Yunjie Qian","userId":"16160804486899806073"}},"outputId":"4f66ee4a-c78d-4f98-83f5-b68471c52a7e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c77359dc32784f80a2cc71afa805d83e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["['bert.embeddings.word_embeddings.weight',\n"," 'bert.embeddings.position_embeddings.weight',\n"," 'bert.embeddings.token_type_embeddings.weight',\n"," 'bert.embeddings.LayerNorm.weight',\n"," 'bert.embeddings.LayerNorm.bias',\n"," 'bert.encoder.layer.0.attention.self.query.weight',\n"," 'bert.encoder.layer.0.attention.self.query.bias',\n"," 'bert.encoder.layer.0.attention.self.key.weight',\n"," 'bert.encoder.layer.0.attention.self.key.bias',\n"," 'bert.encoder.layer.0.attention.self.value.weight',\n"," 'bert.encoder.layer.0.attention.self.value.bias',\n"," 'bert.encoder.layer.0.attention.output.dense.weight',\n"," 'bert.encoder.layer.0.attention.output.dense.bias',\n"," 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.0.intermediate.dense.weight',\n"," 'bert.encoder.layer.0.intermediate.dense.bias',\n"," 'bert.encoder.layer.0.output.dense.weight',\n"," 'bert.encoder.layer.0.output.dense.bias',\n"," 'bert.encoder.layer.0.output.LayerNorm.weight',\n"," 'bert.encoder.layer.0.output.LayerNorm.bias',\n"," 'bert.encoder.layer.1.attention.self.query.weight',\n"," 'bert.encoder.layer.1.attention.self.query.bias',\n"," 'bert.encoder.layer.1.attention.self.key.weight',\n"," 'bert.encoder.layer.1.attention.self.key.bias',\n"," 'bert.encoder.layer.1.attention.self.value.weight',\n"," 'bert.encoder.layer.1.attention.self.value.bias',\n"," 'bert.encoder.layer.1.attention.output.dense.weight',\n"," 'bert.encoder.layer.1.attention.output.dense.bias',\n"," 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.1.intermediate.dense.weight',\n"," 'bert.encoder.layer.1.intermediate.dense.bias',\n"," 'bert.encoder.layer.1.output.dense.weight',\n"," 'bert.encoder.layer.1.output.dense.bias',\n"," 'bert.encoder.layer.1.output.LayerNorm.weight',\n"," 'bert.encoder.layer.1.output.LayerNorm.bias',\n"," 'bert.encoder.layer.2.attention.self.query.weight',\n"," 'bert.encoder.layer.2.attention.self.query.bias',\n"," 'bert.encoder.layer.2.attention.self.key.weight',\n"," 'bert.encoder.layer.2.attention.self.key.bias',\n"," 'bert.encoder.layer.2.attention.self.value.weight',\n"," 'bert.encoder.layer.2.attention.self.value.bias',\n"," 'bert.encoder.layer.2.attention.output.dense.weight',\n"," 'bert.encoder.layer.2.attention.output.dense.bias',\n"," 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.2.intermediate.dense.weight',\n"," 'bert.encoder.layer.2.intermediate.dense.bias',\n"," 'bert.encoder.layer.2.output.dense.weight',\n"," 'bert.encoder.layer.2.output.dense.bias',\n"," 'bert.encoder.layer.2.output.LayerNorm.weight',\n"," 'bert.encoder.layer.2.output.LayerNorm.bias',\n"," 'bert.encoder.layer.3.attention.self.query.weight',\n"," 'bert.encoder.layer.3.attention.self.query.bias',\n"," 'bert.encoder.layer.3.attention.self.key.weight',\n"," 'bert.encoder.layer.3.attention.self.key.bias',\n"," 'bert.encoder.layer.3.attention.self.value.weight',\n"," 'bert.encoder.layer.3.attention.self.value.bias',\n"," 'bert.encoder.layer.3.attention.output.dense.weight',\n"," 'bert.encoder.layer.3.attention.output.dense.bias',\n"," 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.3.intermediate.dense.weight',\n"," 'bert.encoder.layer.3.intermediate.dense.bias',\n"," 'bert.encoder.layer.3.output.dense.weight',\n"," 'bert.encoder.layer.3.output.dense.bias',\n"," 'bert.encoder.layer.3.output.LayerNorm.weight',\n"," 'bert.encoder.layer.3.output.LayerNorm.bias',\n"," 'bert.encoder.layer.4.attention.self.query.weight',\n"," 'bert.encoder.layer.4.attention.self.query.bias',\n"," 'bert.encoder.layer.4.attention.self.key.weight',\n"," 'bert.encoder.layer.4.attention.self.key.bias',\n"," 'bert.encoder.layer.4.attention.self.value.weight',\n"," 'bert.encoder.layer.4.attention.self.value.bias',\n"," 'bert.encoder.layer.4.attention.output.dense.weight',\n"," 'bert.encoder.layer.4.attention.output.dense.bias',\n"," 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.4.intermediate.dense.weight',\n"," 'bert.encoder.layer.4.intermediate.dense.bias',\n"," 'bert.encoder.layer.4.output.dense.weight',\n"," 'bert.encoder.layer.4.output.dense.bias',\n"," 'bert.encoder.layer.4.output.LayerNorm.weight',\n"," 'bert.encoder.layer.4.output.LayerNorm.bias',\n"," 'bert.encoder.layer.5.attention.self.query.weight',\n"," 'bert.encoder.layer.5.attention.self.query.bias',\n"," 'bert.encoder.layer.5.attention.self.key.weight',\n"," 'bert.encoder.layer.5.attention.self.key.bias',\n"," 'bert.encoder.layer.5.attention.self.value.weight',\n"," 'bert.encoder.layer.5.attention.self.value.bias',\n"," 'bert.encoder.layer.5.attention.output.dense.weight',\n"," 'bert.encoder.layer.5.attention.output.dense.bias',\n"," 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.5.intermediate.dense.weight',\n"," 'bert.encoder.layer.5.intermediate.dense.bias',\n"," 'bert.encoder.layer.5.output.dense.weight',\n"," 'bert.encoder.layer.5.output.dense.bias',\n"," 'bert.encoder.layer.5.output.LayerNorm.weight',\n"," 'bert.encoder.layer.5.output.LayerNorm.bias',\n"," 'bert.encoder.layer.6.attention.self.query.weight',\n"," 'bert.encoder.layer.6.attention.self.query.bias',\n"," 'bert.encoder.layer.6.attention.self.key.weight',\n"," 'bert.encoder.layer.6.attention.self.key.bias',\n"," 'bert.encoder.layer.6.attention.self.value.weight',\n"," 'bert.encoder.layer.6.attention.self.value.bias',\n"," 'bert.encoder.layer.6.attention.output.dense.weight',\n"," 'bert.encoder.layer.6.attention.output.dense.bias',\n"," 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.6.intermediate.dense.weight',\n"," 'bert.encoder.layer.6.intermediate.dense.bias',\n"," 'bert.encoder.layer.6.output.dense.weight',\n"," 'bert.encoder.layer.6.output.dense.bias',\n"," 'bert.encoder.layer.6.output.LayerNorm.weight',\n"," 'bert.encoder.layer.6.output.LayerNorm.bias',\n"," 'bert.encoder.layer.7.attention.self.query.weight',\n"," 'bert.encoder.layer.7.attention.self.query.bias',\n"," 'bert.encoder.layer.7.attention.self.key.weight',\n"," 'bert.encoder.layer.7.attention.self.key.bias',\n"," 'bert.encoder.layer.7.attention.self.value.weight',\n"," 'bert.encoder.layer.7.attention.self.value.bias',\n"," 'bert.encoder.layer.7.attention.output.dense.weight',\n"," 'bert.encoder.layer.7.attention.output.dense.bias',\n"," 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.7.intermediate.dense.weight',\n"," 'bert.encoder.layer.7.intermediate.dense.bias',\n"," 'bert.encoder.layer.7.output.dense.weight',\n"," 'bert.encoder.layer.7.output.dense.bias',\n"," 'bert.encoder.layer.7.output.LayerNorm.weight',\n"," 'bert.encoder.layer.7.output.LayerNorm.bias',\n"," 'bert.encoder.layer.8.attention.self.query.weight',\n"," 'bert.encoder.layer.8.attention.self.query.bias',\n"," 'bert.encoder.layer.8.attention.self.key.weight',\n"," 'bert.encoder.layer.8.attention.self.key.bias',\n"," 'bert.encoder.layer.8.attention.self.value.weight',\n"," 'bert.encoder.layer.8.attention.self.value.bias',\n"," 'bert.encoder.layer.8.attention.output.dense.weight',\n"," 'bert.encoder.layer.8.attention.output.dense.bias',\n"," 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.8.intermediate.dense.weight',\n"," 'bert.encoder.layer.8.intermediate.dense.bias',\n"," 'bert.encoder.layer.8.output.dense.weight',\n"," 'bert.encoder.layer.8.output.dense.bias',\n"," 'bert.encoder.layer.8.output.LayerNorm.weight',\n"," 'bert.encoder.layer.8.output.LayerNorm.bias',\n"," 'bert.encoder.layer.9.attention.self.query.weight',\n"," 'bert.encoder.layer.9.attention.self.query.bias',\n"," 'bert.encoder.layer.9.attention.self.key.weight',\n"," 'bert.encoder.layer.9.attention.self.key.bias',\n"," 'bert.encoder.layer.9.attention.self.value.weight',\n"," 'bert.encoder.layer.9.attention.self.value.bias',\n"," 'bert.encoder.layer.9.attention.output.dense.weight',\n"," 'bert.encoder.layer.9.attention.output.dense.bias',\n"," 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.9.intermediate.dense.weight',\n"," 'bert.encoder.layer.9.intermediate.dense.bias',\n"," 'bert.encoder.layer.9.output.dense.weight',\n"," 'bert.encoder.layer.9.output.dense.bias',\n"," 'bert.encoder.layer.9.output.LayerNorm.weight',\n"," 'bert.encoder.layer.9.output.LayerNorm.bias',\n"," 'bert.encoder.layer.10.attention.self.query.weight',\n"," 'bert.encoder.layer.10.attention.self.query.bias',\n"," 'bert.encoder.layer.10.attention.self.key.weight',\n"," 'bert.encoder.layer.10.attention.self.key.bias',\n"," 'bert.encoder.layer.10.attention.self.value.weight',\n"," 'bert.encoder.layer.10.attention.self.value.bias',\n"," 'bert.encoder.layer.10.attention.output.dense.weight',\n"," 'bert.encoder.layer.10.attention.output.dense.bias',\n"," 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.10.intermediate.dense.weight',\n"," 'bert.encoder.layer.10.intermediate.dense.bias',\n"," 'bert.encoder.layer.10.output.dense.weight',\n"," 'bert.encoder.layer.10.output.dense.bias',\n"," 'bert.encoder.layer.10.output.LayerNorm.weight',\n"," 'bert.encoder.layer.10.output.LayerNorm.bias',\n"," 'bert.encoder.layer.11.attention.self.query.weight',\n"," 'bert.encoder.layer.11.attention.self.query.bias',\n"," 'bert.encoder.layer.11.attention.self.key.weight',\n"," 'bert.encoder.layer.11.attention.self.key.bias',\n"," 'bert.encoder.layer.11.attention.self.value.weight',\n"," 'bert.encoder.layer.11.attention.self.value.bias',\n"," 'bert.encoder.layer.11.attention.output.dense.weight',\n"," 'bert.encoder.layer.11.attention.output.dense.bias',\n"," 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.11.intermediate.dense.weight',\n"," 'bert.encoder.layer.11.intermediate.dense.bias',\n"," 'bert.encoder.layer.11.output.dense.weight',\n"," 'bert.encoder.layer.11.output.dense.bias',\n"," 'bert.encoder.layer.11.output.LayerNorm.weight',\n"," 'bert.encoder.layer.11.output.LayerNorm.bias',\n"," 'bert.pooler.dense.weight',\n"," 'bert.pooler.dense.bias',\n"," 'classifier.weight',\n"," 'classifier.bias']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uuLoJgsDGQcL","executionInfo":{"status":"ok","timestamp":1680212405742,"user_tz":240,"elapsed":12,"user":{"displayName":"Yunjie Qian","userId":"16160804486899806073"}},"outputId":"e23d7f3e-8cdb-4b12-e1ed-4c9b5bf5a641"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The BERT model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                  (30522, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n"]}]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 3\n","learning_rate = 5e-5\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n","total_steps = (len(train_dataloader)) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"],"metadata":{"id":"-2qPGWHzGQgR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_accuracy(preds, labels):\n","    pred = torch.argmax(preds, axis=1).squeeze()\n","    true_label = labels.squeeze()\n","    return torch.sum(pred == true_label) / len(true_label)"],"metadata":{"id":"1xqVs5UtN2Lh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(epochs):\n","\n","    print(f'Epoch {epoch + 1}/{epochs}')\n","\n","    model.train()\n","    total_loss = 0\n","\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        \n","        optimizer.zero_grad()\n","\n","        outputs = model(**batch)\n","        loss, logits = outputs.loss, outputs.logits\n","        total_loss += loss.item()\n","        loss.backward()\n","        \n","        optimizer.step()\n","        scheduler.step()\n","    \n","    avg_train_loss = total_loss / len(train_dataloader)\n","    print(f'Training loss: {avg_train_loss}')\n","    \n","    \n","    model.eval()\n","    total_val_loss, total_val_accuracy = 0, 0\n","    \n","    with torch.no_grad():\n","        \n","        for batch in validation_dataloader:\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**batch)\n","            loss, logits = outputs.loss, outputs.logits\n","            total_val_loss += loss.item()\n","            total_val_accuracy += calculate_accuracy(logits, batch['labels'])\n","        \n","        avg_val_loss = total_val_loss / len(validation_dataloader)\n","        avg_val_accuracy = total_val_accuracy / len(validation_dataloader)\n","        print(f'Validation loss: {avg_val_loss}')\n","        print(f'Validation Accuracy: {avg_val_accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tXbkG2cGGQkm","executionInfo":{"status":"ok","timestamp":1680212621164,"user_tz":240,"elapsed":212079,"user":{"displayName":"Yunjie Qian","userId":"16160804486899806073"}},"outputId":"43db6120-65b6-4b6e-df14-bc8c43af2628"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","Training loss: 0.4886023051016558\n","Validation loss: 0.4922972685760922\n","Validation Accuracy: 0.7974537014961243\n","Epoch 2/3\n","Training loss: 0.2546563266594875\n","Validation loss: 0.41918608435878046\n","Validation Accuracy: 0.8182870149612427\n","Epoch 3/3\n","Training loss: 0.10258493121883434\n","Validation loss: 0.6259958285976339\n","Validation Accuracy: 0.8128857612609863\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vTHkShCpGQoe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Cawk5BibH9ph"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import time, datetime\n","\n","# def format_time(elapsed):\n","#     elapsed_rounded = int(round((elapsed)))\n","#     return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"hU0cTTvNH9ub"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # We'll store a number of quantities such as training and validation loss, \n","# # validation accuracy, and timings.\n","# training_stats = []\n","\n","# # Measure the total training time for the whole run.\n","# total_t0 = time.time()\n","\n","# # For each epoch...\n","# for epoch_i in range(0, epochs):\n","    \n","#     # ========================================\n","#     #               Training\n","#     # ========================================\n","    \n","#     # Perform one full pass over the training set.\n","\n","#     print(\"\")\n","#     print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","#     print('Training...')\n","\n","#     # Measure how long the training epoch takes.\n","#     t0 = time.time()\n","\n","#     # Reset the total loss for this epoch.\n","#     total_train_loss = 0\n","\n","#     # Put the model into training mode.\n","#     model.train()\n","\n","#     # For each batch of training data...\n","#     for step, batch in enumerate(train_dataloader):\n","\n","#         # Progress update every 40 batches.\n","#         if step % 40 == 0 and not step == 0:\n","#             # Calculate elapsed time in minutes.\n","#             elapsed = format_time(time.time() - t0)\n","#             # Report progress.\n","#             print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","#         # `batch` contains three pytorch tensors:\n","#         # [0]: input ids, [1]: attention masks, [2]: labels \n","#         b_input_ids = batch[0].to(device)\n","#         b_input_mask = batch[1].to(device)\n","#         b_labels = batch[2].to(device)\n","#         print(b_input_ids.shape, b_input_mask.shape, b_labels.shape)\n","\n","#         # Always clear any previously calculated gradients before performing a backward pass. \n","#         # PyTorch doesn't do this automatically because accumulating the gradients is \"convenient while training RNNs\". \n","#         # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","#         model.zero_grad()        \n","\n","#         # Perform a forward pass (evaluate the model on this training batch).\n","#         # The documentation for this `model` function is here: \n","#         # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","#         # It returns different numbers of parameters depending on what arguments are given and what flags are set. \n","#         # For our usage here, it returns the loss (because we provided labels) and the \"logits\"--the model outputs prior to activation.\n","#         loss, logits = model(b_input_ids, \n","#                              token_type_ids = None, \n","#                              attention_mask = b_input_mask, \n","#                              labels = b_labels)\n","\n","#         print(loss, logits)\n","#         # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end. \n","#         # `loss` is a Tensor containing a single value; the `.item()` function just returns the Python value from the tensor.\n","#         total_train_loss += loss.item()\n","\n","#         # Perform a backward pass to calculate the gradients.\n","#         loss.backward()\n","\n","#         # Clip the norm of the gradients to 1.0.\n","#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","#         optimizer.step()\n","#         scheduler.step()\n","\n","#     # Calculate the average loss over all of the batches.\n","#     avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","#     # Measure how long this epoch took.\n","#     training_time = format_time(time.time() - t0)\n","\n","#     print(\"\")\n","#     print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","#     print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    \n","#     # ========================================\n","#     #               Validation\n","#     # ========================================\n","#     # After the completion of each training epoch, measure our performance on\n","#     # our validation set.\n","\n","#     print(\"\")\n","#     print(\"Running Validation...\")\n","\n","#     t0 = time.time()\n","\n","#     # Put the model in evaluation mode.\n","#     model.eval()\n","\n","#     # Tracking variables \n","#     total_eval_accuracy = 0\n","#     total_eval_loss = 0\n","#     nb_eval_steps = 0\n","\n","#     # Evaluate data for one epoch\n","#     for batch in validation_dataloader:\n","        \n","#         # `batch` contains three pytorch tensors:\n","#         # [0]: input ids, [1]: attention masks, [2]: labels \n","#         b_input_ids = batch[0].to(device)\n","#         b_input_mask = batch[1].to(device)\n","#         b_labels = batch[2].to(device)\n","        \n","#         with torch.no_grad():        \n","\n","#             # Forward pass, calculate logit predictions.\n","#             # token_type_ids is the same as the \"segment ids\", which differentiates sentence 1 and 2 in 2-sentence tasks.\n","#             # The documentation for this `model` function is here: \n","#             # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","#             # Get the \"logits\" output by the model. The \"logits\" are the output values prior to applying an activation function like the softmax.\n","#             loss, logits = model(b_input_ids, \n","#                                  token_type_ids = None, \n","#                                  attention_mask = b_input_mask,\n","#                                  labels = b_labels)\n","            \n","#             # Accumulate the validation loss.\n","#             total_eval_loss += loss.item()\n","\n","#             # Move logits and labels to CPU\n","#             logits = logits.detach().cpu().numpy()\n","#             label_ids = b_labels.to('cpu').numpy()\n","\n","#             # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n","#             total_eval_accuracy += calculate_accuracy(logits, label_ids)\n","\n","#             # Report the final accuracy for this validation run.\n","#             avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","#             print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","#             # Calculate the average loss over all of the batches.\n","#             avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","#     # Measure how long the validation run took.\n","#     validation_time = format_time(time.time() - t0)\n","    \n","#     print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","#     print(\"  Validation took: {:}\".format(validation_time))\n","\n","#     # Record all statistics from this epoch.\n","#     training_stats.append(\n","#         {\n","#             'epoch': epoch_i + 1,\n","#             'Training Loss': avg_train_loss,\n","#             'Valid. Loss': avg_val_loss,\n","#             'Valid. Accur.': avg_val_accuracy,\n","#             'Training Time': training_time,\n","#             'Validation Time': validation_time\n","#         }\n","#     )\n","\n","# print(\"\")\n","# print(\"Training complete!\")\n","\n","# print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"metadata":{"id":"h0MwIcnCIdJH"},"execution_count":null,"outputs":[]}]}