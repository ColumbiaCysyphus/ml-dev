{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import difflib\n",
    "import random, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tika import parser\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(pdf_content):\n",
    "    punc = ['• ', '· ', '&', '~', ' o ', '\\uf0a7', '\\uf03c', '\\uf0b7', \n",
    "            '–', '()', '[…]', '| ', '© ', '(Insert Scale)', '_', '%', '[', ']', 'Ü ']\n",
    "    for p in punc:\n",
    "        pdf_content = pdf_content.replace(p, '')\n",
    "    return pdf_content\n",
    "\n",
    "def remove_bulleted_points(pdf_content):\n",
    "    pdf_content = re.sub(r'\\.+ [0-9]+', '.', pdf_content)\n",
    "    pdf_content = re.sub(r'\\.+[0-9]+', '.', pdf_content)\n",
    "    pdf_content = re.sub(r'\\.+', '.', pdf_content)\n",
    "\n",
    "    pdf_content = re.sub(r'\\([0-9]+\\)', '', pdf_content)\n",
    "    pdf_content = re.sub(r'[0-9]+\\)', '', pdf_content)\n",
    "    pdf_content = re.sub(r'[0-9]+.', '', pdf_content)\n",
    "    pdf_content = re.sub(r'\\([a-zA-Z]\\)', '', pdf_content)\n",
    "    pdf_content = re.sub(r' [a-zA-Z]\\)', '', pdf_content)\n",
    "    pdf_content = re.sub(r'\\(i+\\)', '', pdf_content)\n",
    "    pdf_content = re.sub(r' i+\\)', '', pdf_content)\n",
    "\n",
    "    pdf_content = re.sub('\\s\\s+', ' ', pdf_content)\n",
    "    return pdf_content\n",
    "\n",
    "def remove_url(pdf_content):\n",
    "    url = re.findall('http[s]?://\\S+', pdf_content)\n",
    "    for u in url:\n",
    "        pdf_content = pdf_content.replace(u, '')\n",
    "    url = re.findall('www.\\S+', pdf_content)\n",
    "    for u in url:\n",
    "        pdf_content = pdf_content.replace(u, '')\n",
    "    pdf_content = re.sub(r'http[s]?://', '', pdf_content)\n",
    "    return pdf_content\n",
    "\n",
    "def filter_sentences_by_length(pdf_sentence):\n",
    "    return [s for s in pdf_sentence if len(word_tokenize(s)) > 4 and len(word_tokenize(s)) < 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3773/1776873072.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only.\n",
      "  recs = pd.read_csv('cleaned_recs.csv')[['Document File Name ', 'Recommendation text']].dropna(0, 'all')\n"
     ]
    }
   ],
   "source": [
    "recs = pd.read_csv('cleaned_recs.csv')[['Document File Name ', 'Recommendation text']].dropna(0, 'all')\n",
    "file_mapping = pd.read_csv(\"file_mapping.csv\")\n",
    "merged = pd.merge(recs, file_mapping, left_on=\"Document File Name \", right_on=\"original_name\", how='inner')\n",
    "test = merged.loc[(merged.indexed_name == '12.pdf') | (merged.indexed_name == '9.pdf')]\n",
    "train = merged.loc[~((merged.indexed_name == '12.pdf') | (merged.indexed_name == '9.pdf'))]\n",
    "\n",
    "# sentences = []\n",
    "# indexed_corpus = os.path.join(\"..\", \"indexed_corpus\")\n",
    "# for i in range(1, 16):\n",
    "    \n",
    "#     pdf_path = os.path.join(indexed_corpus, f\"{i}.pdf\")\n",
    "#     parsed_pdf = parser.from_file(pdf_path)\n",
    "#     pdf_content = parsed_pdf['content'].replace('\\n', ' ').replace(';', '.').strip()\n",
    "#     pdf_content = remove_punc(pdf_content)\n",
    "#     pdf_content = remove_bulleted_points(pdf_content)\n",
    "#     pdf_content = remove_url(pdf_content)\n",
    "#     pdf_content = remove_punc(pdf_content)\n",
    "#     pdf_content = re.sub(r'\\.+', '.', pdf_content)\n",
    "#     pdf_content = re.sub(r'\\s\\s+', ' ', pdf_content)\n",
    "    \n",
    "#     pdf_sentence = sent_tokenize(pdf_content)\n",
    "#     filtered_sentence = filter_sentences_by_length(pdf_sentence)\n",
    "#     sentences += filtered_sentence\n",
    "\n",
    "# len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_pickle(data, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "save_list_to_pickle(merged['Recommendation text'].to_list(), \"recs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list_from_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "filename = \"sentences.pkl\"\n",
    "sentences = load_list_from_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_sentence_index(sentence, sentence_list):\n",
    "    # Tokenize the sentences\n",
    "    sentence_tokens = sentence.split()\n",
    "    sentence_list_tokens = [s.split() for s in sentence_list]\n",
    "    \n",
    "    # Calculate the similarity between the sentences\n",
    "    similarity_scores = [difflib.SequenceMatcher(None, sentence_tokens, s).ratio() for s in sentence_list_tokens]\n",
    "    \n",
    "    # Find the index of the most similar sentence\n",
    "    max_similarity_index = similarity_scores.index(max(similarity_scores))\n",
    "    \n",
    "    return max_similarity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = [retrieve_sentence_index(sentence, sentences) for sentence in train.iloc[:, 1]]\n",
    "test_indices = [retrieve_sentence_index(sentence, sentences) for sentence in test.iloc[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_indices), len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recs = [sentences[idx] for idx in set(train_indices)]\n",
    "test_recs = [sentences[idx] for idx in set(test_indices)]\n",
    "\n",
    "non_recs = []\n",
    "while len(non_recs) != 125:\n",
    "    samp_idx = np.random.choice(len(sentences))\n",
    "    if (samp_idx not in train_indices + test_indices) and (len(sentences[samp_idx].split()) > 10):\n",
    "        non_recs.append(sentences[samp_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_non_recs = non_recs[:110]\n",
    "test_non_recs = non_recs[110:]\n",
    "\n",
    "train_texts = train_non_recs + train_recs\n",
    "test_texts = test_non_recs + test_recs\n",
    "\n",
    "train_labels = [0] * len(train_non_recs) + [1] * len(train_recs)\n",
    "test_labels = [0] * len(test_non_recs) + [1] * len(test_recs)\n",
    "\n",
    "train_combined = list(zip(train_texts, train_labels))\n",
    "test_combined = list(zip(test_texts, test_labels))\n",
    "\n",
    "# # Shuffle the combined lists\n",
    "# random.shuffle(train_combined)\n",
    "# random.shuffle(test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/shubham/anaconda3/envs/ancestry/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Unzip the shuffled combined lists\n",
    "texts, labels = zip(*train_combined)\n",
    "test_texts, test_labels = zip(*test_combined)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define the dataset and data loaders\n",
    "train_dataset = RecsDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = RecsDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "test_dataset = RecsDataset(test_texts, test_labels, tokenizer, max_length=128)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "epochs = 25\n",
    "\n",
    "# Create logger\n",
    "\n",
    "f = open(os.path.join(\"logs\", \"baseline_logger1.txt\"), 'w')\n",
    "\n",
    "best_val_loss = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        loss = loss_fn(logits, labels)\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_accuracy = 100.0 * train_correct / len(train_dataset)\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            \n",
    "            loss = loss_fn(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100.0 * val_correct / len(val_dataset)\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), os.path.join(\"weights\", 'baseline1.pt'))\n",
    "        best_val_loss = val_loss    \n",
    "\n",
    "    f.write(f'Epoch {epoch + 1}/{epochs}\\n')\n",
    "    f.write(f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%\\n')\n",
    "    f.write(f'Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%\\n')\n",
    "    f.write('-------------------------------------------\\n')\n",
    "\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load(os.path.join(\"weights\", 'baseline1.pt')))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "# test_correct = 0\n",
    "\n",
    "all_predicted_labels = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        loss = loss_fn(logits, labels)\n",
    "        test_loss += loss.item()\n",
    "        # test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predicted_labels.extend(predicted.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = accuracy_score(all_true_labels, all_predicted_labels) * 100\n",
    "test_precision = precision_score(all_true_labels, all_predicted_labels)\n",
    "test_recall = recall_score(all_true_labels, all_predicted_labels)\n",
    "\n",
    "test_loss /= len(test_dataloader)\n",
    "\n",
    "f.write(f'TESTING\\n')\n",
    "f.write(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.2%}\\n')\n",
    "f.write(f'Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.2f}\\n')\n",
    "f.write('-------------------------------------------\\n')\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation (Back-translation // TF-IDF Replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ancestry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
